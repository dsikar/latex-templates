\section{Neural Networks}

The term "neural network" refers to a family of algorithms inspired by models of the brain.
Such models are employed in statistics, cognitive psychology and artificial intelligence.
Some models seek to emulate the central nervous system: computational neuroscience.
Others (deep networks) have become very popular recently in large-scale data analysis
(data science), particularly in image, video, audio and language modelling \cite{Garcez2019}. 


\subsection{A Brief History of Neural Networks}

\begin{itemize}
    \item
1940s
– McCulloch and Pitts neuron (biological motivation)
    \item
1950s
– Hebbian learning (first learning algorithm by
Donald Hebb)
– Frank Rosenblatt’s Perceptron (neural net for
pattern recognition)
    \item
1960s
– Widrow-Hoff learning rule (similar to perceptron’s)
First application: used in signal processing, e.g.
echo cancellation in phone lines
    \item
1969
– Minsky and Papert’s critique of perceptron (shows
perceptron’s linear separability problem, and
practically halts all NN research for twenty years)
    \item
1987
– Backpropagation algorithm (overcomes the
limitations of perceptron)
    \item
1990s
– Powerful computers
– Statistical foundations of neural nets established

    \item
2000s
– A standard tool to solve many practical problems
– Logical foundations of neural nets established
– Biological motivation stronger than ever (fMRI)
– New insights into Consciousness
    \item
2010s
– Big Data and Deep networks: fast algorithms to
train large nets (state-of-the-art speech and image
recognition and language translation)
– Social computing, human-like computation (see
HCOMP conference series)
– Human Brain Projects (US and EU)
\end{itemize}



